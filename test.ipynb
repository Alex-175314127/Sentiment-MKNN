{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models.MKNN import ModifiedKNN\n",
    "import neattext.functions as nfx\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Twitter_fresh/twitter_crawling.csv',encoding='latin1', usecols=['date','text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casefolding(Text):\n",
    "    Text = Text.lower()\n",
    "    return Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(casefolding)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_clean(Text):\n",
    "        Text = nfx.remove_urls(Text)\n",
    "        Text = nfx.remove_punctuations(Text)\n",
    "        Text = nfx.remove_emojis(Text)\n",
    "        Text = nfx.remove_special_characters(Text)\n",
    "        Text = nfx.remove_numbers(Text)\n",
    "        return Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(punc_clean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize_wrapper(Text):\n",
    "        return word_tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(word_tokenize_wrapper)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_norm(tweets):\n",
    "    word_dict = pd.read_csv('data/indonesia_slangWords.csv')\n",
    "    norm_word_dict = {}\n",
    "    for index, row in word_dict.iterrows():\n",
    "        if row[0] not in norm_word_dict:\n",
    "            norm_word_dict[row[0]] = row[1]\n",
    "    return [norm_word_dict[term] if term in norm_word_dict else term for term in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(word_norm)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(Text):\n",
    "    stopW = stopwords.words('indonesian', 'english')\n",
    "    sw = pd.read_csv('data/stopwordbahasa.csv')\n",
    "    stopW.extend(sw)\n",
    "    remove_sw = ' '.join(Text)\n",
    "    clean_sw = [word for word in remove_sw.split() if word.lower() not in stopW]\n",
    "    return clean_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(remove_stopword)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indo_stem(Text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    result = []\n",
    "    for w in Text:\n",
    "        result.append(stemmer.stem(w))\n",
    "        result.append(\" \")\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(indo_stem)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentiment_analysis():\n",
    "    df = pd.read_csv('Twitter_fresh\\Clean Data\\Twitter_clean_1000.csv', encoding='utf-8')\n",
    "\n",
    "    # LexiconVader dengan custom Lexicon(bahasa indonesia)\n",
    "    sia1A, sia1B = SentimentIntensityAnalyzer(), SentimentIntensityAnalyzer()\n",
    "    senti = SentimentIntensityAnalyzer()\n",
    "    # Hapus Default lexicon VADER\n",
    "    sia1A.lexicon.clear()\n",
    "    sia1B.lexicon.clear()\n",
    "    senti.lexicon.clear()\n",
    "\n",
    "    # Read custom Lexicon Bahasa Indonesia\n",
    "    data1A = open('data/lexicon_sentimen_negatif.txt', 'r').read()\n",
    "    data1B = open('data/lexicon_sentimen_positif.txt', 'r').read()\n",
    "    data_senti = open('data/sentiwords_id.txt', 'r').read()\n",
    "    \n",
    "    # convert lexicon to dictonary\n",
    "    insetNeg = json.loads(data1A)\n",
    "    insetPos = json.loads(data1B)\n",
    "    sa = json.loads(data_senti)\n",
    "\n",
    "    # update lexicon vader with custom lexicon (b.indo)\n",
    "    sia1A.lexicon.update(insetNeg)\n",
    "    sia1B.lexicon.update(insetPos)\n",
    "    senti.lexicon.update(sa)\n",
    "\n",
    "    # method untuk cek apa sentimen pos,neg,neu\n",
    "    def is_positive_inset(Text: str) -> bool:\n",
    "        return sia1A.polarity_scores(Text)[\"compound\"] + sia1B.polarity_scores(Text)[\"compound\"] >= 0.05\n",
    "    \n",
    "    tweets = df['text'].to_list()\n",
    "\n",
    "    with open('output/Sentiment-result.txt', 'w+') as f:\n",
    "        for tweet in tweets:\n",
    "            label = \"Positive\" if is_positive_inset(tweet) else \"Negative\"\n",
    "            f.write(str(label + \"\\n\"))\n",
    "\n",
    "    sen = pd.read_csv('output/Sentiment-result.txt', names=['Sentiment'])\n",
    "    df = df.join(sen)\n",
    "\n",
    "    ## Save clean Dataset\n",
    "    #df.to_csv('CleanText_Sentiment.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_result = Sentiment_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    791\n",
       "Positive    209\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_result['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_result.to_csv('output/Sentiment_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from heapq import nsmallest as nMin\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:03,  1.16it/s]c:\\Users\\Alexander Adam\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "10it [00:08,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "k_value = 9 # 1 - 25\n",
    "X = sentiment_result['text'].values\n",
    "y = sentiment_result['Sentiment'].values\n",
    "fold_i = 1\n",
    "fold_n = 10 # 3 5 7 10\n",
    "sum_accuracy = 0\n",
    "kfold = KFold(fold_n, shuffle=True, random_state=42)\n",
    "enc = LabelEncoder()\n",
    "fol = []\n",
    "acc, rc, pr, f1 = [], [], [], []\n",
    "\n",
    "for train_index, test_index in tqdm(kfold.split(X)):\n",
    "    fol.append(fold_i)\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    svf = open('output/ResultX.txt', 'w')\n",
    "    sv_text = '\\n'.join(str(item) for item in X_test).replace(\"   \",\" \")\n",
    "    svf.write(sv_text)\n",
    "    svY = open ('output/y_train.txt', 'w')\n",
    "    svY.write('\\n'.join(str(item) for item in y_train))\n",
    "\n",
    "    #TFIDF\n",
    "    tf = TfidfVectorizer(decode_error=\"replace\")\n",
    "    X_train = tf.fit_transform(X_train)\n",
    "    X_test = tf.transform(X_test)\n",
    "    \n",
    "    y_train = enc.fit_transform(y_train)\n",
    "    y_test = enc.transform(y_test)\n",
    "\n",
    "    # Algorithm\n",
    "    clf = ModifiedKNN(k_value)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred, jarak = clf.predict(X_test)\n",
    "    neigbor_index = clf.get_neigbors(X_test)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    #tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    accuracy = accuracy_score(y_test, pred)*100\n",
    "    precision = precision_score(y_test, pred)*100\n",
    "    recall = recall_score(y_test, pred)*100\n",
    "    f1_scores = f1_score(y_test, pred)*100\n",
    "    #accuracy = (tp + tn) / (tp + fp + tn + fn)*100\n",
    "    #precision = (tp) / (tp + fp)*100\n",
    "    #recall = (tp) / (tp + fn)*100\n",
    "    #f1_scores = (2 * precision * recall) / (precision + recall)\n",
    "    #plot_conf_metrics(y_test, pred)\n",
    "\n",
    "    sum_accuracy += accuracy\n",
    "    pred = enc.inverse_transform(pred)\n",
    "\n",
    "    fold_i += 1\n",
    "    acc.append(accuracy)\n",
    "    pr.append(precision)\n",
    "    rc.append(recall)\n",
    "    f1.append(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/MKNN_prediction.txt\", \"w\") as f:\n",
    "    mknn_predited_label ='\\n'.join(str(item) for item in pred)\n",
    "    f.write(mknn_predited_label)\n",
    "with open('output/jarak_ttg.txt', 'w') as g:\n",
    "    jarak = [nMin(k_value,map(float,i)) for i in jarak]\n",
    "    mknn_distance = '\\n'.join(str(ls) for ls in jarak)\n",
    "    g.write(mknn_distance)\n",
    "with open('output/index_ttg.txt', 'w') as j:\n",
    "    j.write('\\n'.join(str(a) for a in neigbor_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = pd.read_csv('output/MKNN_prediction.txt', names=['Sentiment'])\n",
    "jarak_pred = pd.read_csv('output/jarak_ttg.txt', names=['Distance'], sep='\\t')\n",
    "text_test = pd.read_csv('output/ResultX.txt', names=['text'])\n",
    "index_pred = pd.read_csv('output/index_ttg.txt', names=['Neigbor'])\n",
    "text_test = text_test.join(knn_pred)\n",
    "text_test = text_test.join(jarak_pred)\n",
    "text_test = text_test.join(index_pred)\n",
    "#text_test['Sentiment'] = text_test['Sentiment'].apply(lambda x: 'Positive' if x == 1 else 'Negative')\n",
    "text_test = text_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Neigbor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>money heist suka curi</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[1.1165595996858388, 1.13578387669024, 1.16036...</td>\n",
       "      <td>[733 295 701 699 676 474 836   5  65]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebel banget deh nonton film film bagus netfli...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[0.9862296931918912, 1.1235266221003684, 1.154...</td>\n",
       "      <td>[  8 477 796  59 127 694 741 339 873]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mimpi tokyo money heist</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[0.8488482062392781, 0.9300298921025985, 0.930...</td>\n",
       "      <td>[816 496 278 643 465 878 509 407 256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>season nya money heist nonton nafas anjirrr en...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[1.0747805724581887, 1.1554921358307253, 1.229...</td>\n",
       "      <td>[319 281 833 456 424 264 556 468 101]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pikir deh agency tema nya crime agency cari vo...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[1.8250120749944284e-08, 1.2291833293009768, 1...</td>\n",
       "      <td>[ 17 271 330  16  21 273 154 492 217]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text Sentiment  \\\n",
       "0                             money heist suka curi   Negative   \n",
       "1  sebel banget deh nonton film film bagus netfli...  Negative   \n",
       "2                           mimpi tokyo money heist   Negative   \n",
       "3  season nya money heist nonton nafas anjirrr en...  Negative   \n",
       "4  pikir deh agency tema nya crime agency cari vo...  Negative   \n",
       "\n",
       "                                            Distance  \\\n",
       "0  [1.1165595996858388, 1.13578387669024, 1.16036...   \n",
       "1  [0.9862296931918912, 1.1235266221003684, 1.154...   \n",
       "2  [0.8488482062392781, 0.9300298921025985, 0.930...   \n",
       "3  [1.0747805724581887, 1.1554921358307253, 1.229...   \n",
       "4  [1.8250120749944284e-08, 1.2291833293009768, 1...   \n",
       "\n",
       "                                 Neigbor  \n",
       "0  [733 295 701 699 676 474 836   5  65]  \n",
       "1  [  8 477 796  59 127 694 741 339 873]  \n",
       "2  [816 496 278 643 465 878 509 407 256]  \n",
       "3  [319 281 833 456 424 264 556 468 101]  \n",
       "4  [ 17 271 330  16  21 273 154 492 217]  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avearge accuracy :  78.9000%\n",
      "Max Score :  85.0 in Fold :  9\n",
      "Min Score :  75.0 in Fold :  3\n"
     ]
    }
   ],
   "source": [
    "new_frame = pd.DataFrame(X_test)\n",
    "new_frame = new_frame.join(knn_pred)\n",
    "\n",
    "avg_acc = sum_accuracy/fold_n\n",
    "maxs = max(acc)\n",
    "mins = min(acc)\n",
    "res_df = pd.DataFrame({'iterasi':fol, 'Accuracy': acc, 'Precison':pr, 'Recall':rc, 'f1 score':f1})\n",
    "print(\"Avearge accuracy : \", str(\"%.4f\" % avg_acc)+'%')\n",
    "print(\"Max Score : \",str(maxs),\"in Fold : \", str(acc.index(maxs)+1))\n",
    "print(\"Min Score : \",str(mins), \"in Fold : \", str(acc.index(mins)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterasi</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precison</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>76.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>7.692308</td>\n",
       "      <td>13.793103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>77.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>76.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.695652</td>\n",
       "      <td>14.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>78.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>4.347826</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>85.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>11.764706</td>\n",
       "      <td>21.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>82.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>5.882353</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iterasi  Accuracy    Precison     Recall   f1 score\n",
       "0        1      77.0   33.333333   4.545455   8.000000\n",
       "1        2      76.0  100.000000   4.000000   7.692308\n",
       "2        3      75.0   66.666667   7.692308  13.793103\n",
       "3        4      77.0   25.000000   4.761905   8.000000\n",
       "4        5      82.0    0.000000   0.000000   0.000000\n",
       "5        6      81.0    0.000000   0.000000   0.000000\n",
       "6        7      76.0   40.000000   8.695652  14.285714\n",
       "7        8      78.0  100.000000   4.347826   8.333333\n",
       "8        9      85.0  100.000000  11.764706  21.052632\n",
       "9       10      82.0   33.333333   5.882353  10.000000"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[['Accuracy', 'Precison', 'Recall', 'f1 score']].plot.line(title=\"Akurasi tiap Fold\")\n",
    "plot.show(block=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "575b1b4c9cb40b60b65888ca604b6df5d3b9d69fd4f2756bb0ac61f69d2ddd9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
