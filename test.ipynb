{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models.MKNN import ModifiedKNN\n",
    "import neattext.functions as nfx\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Twitter_fresh/twitter_crawling.csv',encoding='latin1', usecols=['date','text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casefolding(Text):\n",
    "    Text = Text.lower()\n",
    "    return Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(casefolding)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_clean(Text):\n",
    "        Text = nfx.remove_urls(Text)\n",
    "        Text = nfx.remove_punctuations(Text)\n",
    "        Text = nfx.remove_emojis(Text)\n",
    "        Text = nfx.remove_special_characters(Text)\n",
    "        Text = nfx.remove_numbers(Text)\n",
    "        return Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(punc_clean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize_wrapper(Text):\n",
    "        return word_tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(word_tokenize_wrapper)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_norm(tweets):\n",
    "    word_dict = pd.read_csv('data/indonesia_slangWords.csv')\n",
    "    norm_word_dict = {}\n",
    "    for index, row in word_dict.iterrows():\n",
    "        if row[0] not in norm_word_dict:\n",
    "            norm_word_dict[row[0]] = row[1]\n",
    "    return [norm_word_dict[term] if term in norm_word_dict else term for term in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(word_norm)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(Text):\n",
    "    stopW = stopwords.words('indonesian', 'english')\n",
    "    sw = pd.read_csv('data/stopwordbahasa.csv')\n",
    "    stopW.extend(sw)\n",
    "    remove_sw = ' '.join(Text)\n",
    "    clean_sw = [word for word in remove_sw.split() if word.lower() not in stopW]\n",
    "    return clean_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(remove_stopword)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indo_stem(Text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    result = []\n",
    "    for w in Text:\n",
    "        result.append(stemmer.stem(w))\n",
    "        result.append(\" \")\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(indo_stem)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentiment_analysis():\n",
    "    df = pd.read_csv('Twitter_fresh\\Clean Data\\Twitter_clean_100.csv', encoding='utf-8')\n",
    "\n",
    "    # LexiconVader dengan custom Lexicon(bahasa indonesia)\n",
    "    sia1A, sia1B = SentimentIntensityAnalyzer(), SentimentIntensityAnalyzer()\n",
    "    # Hapus Default lexicon VADER\n",
    "    sia1A.lexicon.clear()\n",
    "    sia1B.lexicon.clear()\n",
    "\n",
    "    # Read custom Lexicon Bahasa Indonesia\n",
    "    data1A = open('data/lexicon_sentimen_negatif.txt', 'r').read()\n",
    "    data1B = open('data/lexicon_sentimen_positif.txt', 'r').read()\n",
    "    \n",
    "    # convert lexicon to dictonary\n",
    "    insetNeg = json.loads(data1A)\n",
    "    insetPos = json.loads(data1B)\n",
    "\n",
    "    # update lexicon vader with custom lexicon (b.indo)\n",
    "    sia1A.lexicon.update(insetNeg)\n",
    "    sia1B.lexicon.update(insetPos)\n",
    "\n",
    "    # method untuk cek apa sentimen pos,neg,neu\n",
    "    def is_positive_inset(Text: str) -> bool:\n",
    "        return sia1A.polarity_scores(Text)[\"compound\"] + sia1B.polarity_scores(Text)[\"compound\"] > 0\n",
    "    \n",
    "    tweets = df['text'].to_list()\n",
    "\n",
    "    with open('output/Sentiment-result.txt', 'w+') as f:\n",
    "        for tweet in tweets:\n",
    "            label = \"Positive\" if is_positive_inset(tweet) else \"Negative\"\n",
    "            f.write(str(label + \"\\n\"))\n",
    "\n",
    "    sen = pd.read_csv('output/Sentiment-result.txt', names=['Sentiment'])\n",
    "    df = df.join(sen)\n",
    "\n",
    "    ## Save clean Dataset\n",
    "    #df.to_csv('CleanText_Sentiment.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_result = Sentiment_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    72\n",
       "Positive    30\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_result['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_result.to_csv('output/Sentiment_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from heapq import nsmallest as nMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexander Adam\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Alexander Adam\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Alexander Adam\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Alexander Adam\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Alexander Adam\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Alexander Adam\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Alexander Adam\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Alexander Adam\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Alexander Adam\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Alexander Adam\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "k_value = 9 # 1 - 25\n",
    "X = sentiment_result['text'].values\n",
    "y = sentiment_result['Sentiment'].values\n",
    "fold_i = 1\n",
    "fold_n = 10 # 3 5 7 10\n",
    "sum_accuracy = 0\n",
    "kfold = KFold(fold_n, shuffle=True, random_state=42)\n",
    "enc = LabelEncoder()\n",
    "fol = []\n",
    "acc, rc, pr, f1 = [], [], [], []\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    fol.append(fold_i)\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    svf = open('output/ResultX.txt', 'w')\n",
    "    sv_text = '\\n'.join(str(item) for item in X_test).replace(\"   \",\" \")\n",
    "    svf.write(sv_text)\n",
    "    svY = open ('output/y_train.txt', 'w')\n",
    "    svY.write('\\n'.join(str(item) for item in y_train))\n",
    "\n",
    "    #TFIDF\n",
    "    tf = TfidfVectorizer(decode_error=\"replace\")\n",
    "    X_train = tf.fit_transform(X_train)\n",
    "    X_test = tf.transform(X_test)\n",
    "    \n",
    "    y_train = enc.fit_transform(y_train)\n",
    "    y_test = enc.transform(y_test)\n",
    "\n",
    "    # Algorithm\n",
    "    clf = ModifiedKNN(k_value)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred, jarak = clf.predict(X_test)\n",
    "    neigbor_index = clf.get_neigbors(X_test)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    #cm = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)*100\n",
    "    precision = precision_score(y_test, pred)*100\n",
    "    recall = recall_score(y_test, pred)*100\n",
    "    f1_scores = f1_score(y_test, pred)*100\n",
    "    #plot_conf_metrics(y_test, pred)\n",
    "\n",
    "    sum_accuracy += accuracy\n",
    "\n",
    "    fold_i += 1\n",
    "    acc.append(accuracy)\n",
    "    pr.append(precision)\n",
    "    rc.append(recall)\n",
    "    f1.append(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/MKNN_prediction.txt\", \"w\") as f:\n",
    "    mknn_predited_label ='\\n'.join(str(item) for item in pred)\n",
    "    f.write(mknn_predited_label)\n",
    "with open('output/jarak_ttg.txt', 'w') as g:\n",
    "    jarak = [nMin(k_value,map(float,i)) for i in jarak]\n",
    "    mknn_distance = '\\n'.join(str(ls) for ls in jarak)\n",
    "    g.write(mknn_distance)\n",
    "with open('output/index_ttg.txt', 'w') as j:\n",
    "    j.write('\\n'.join(str(a) for a in neigbor_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = pd.read_csv('output/MKNN_prediction.txt', names=['Sentiment'])\n",
    "jarak_pred = pd.read_csv('output/jarak_ttg.txt', names=['Distance'], sep='\\t')\n",
    "text_test = pd.read_csv('output/ResultX.txt', names=['text'])\n",
    "index_pred = pd.read_csv('output/index_ttg.txt', names=['Neigbor'])\n",
    "text_test = text_test.join(knn_pred)\n",
    "text_test = text_test.join(jarak_pred)\n",
    "text_test = text_test.join(index_pred)\n",
    "text_test['Sentiment'] = text_test['Sentiment'].apply(lambda x: 'Positive' if x == 1 else 'Negative')\n",
    "text_test = text_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Neigbor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you akhir episode money heist gagal move on se...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[0.8620227127211233, 0.9750492468940374, 1.050...</td>\n",
       "      <td>[72 39 59 73  4 44 45 41 22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yawla lucu money heist tegang suasana tong sua...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[0.9025295351644446, 1.1446813229959287, 1.149...</td>\n",
       "      <td>[ 4 44 45 41 22 61 54 31 76]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>henti season money heist henti</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[0.8641803066988009, 1.0657919420940885, 1.166...</td>\n",
       "      <td>[78 35  4 33  8 68 51 66 44]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ayah nnton money heist anjing</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[0.8976665940861429, 1.3266930162536887, 1.363...</td>\n",
       "      <td>[ 1  4 44 45 41 22 61 54 31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pantau jam pagi speechless grgr money heist eps</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[1.0562178880200261, 1.2187872576021745, 1.227...</td>\n",
       "      <td>[ 4 44 37 22 28 54 31 33 55]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text Sentiment  \\\n",
       "0  you akhir episode money heist gagal move on se...  Negative   \n",
       "1  yawla lucu money heist tegang suasana tong sua...  Negative   \n",
       "2                    henti season money heist henti   Negative   \n",
       "3                     ayah nnton money heist anjing   Negative   \n",
       "4   pantau jam pagi speechless grgr money heist eps   Negative   \n",
       "\n",
       "                                            Distance  \\\n",
       "0  [0.8620227127211233, 0.9750492468940374, 1.050...   \n",
       "1  [0.9025295351644446, 1.1446813229959287, 1.149...   \n",
       "2  [0.8641803066988009, 1.0657919420940885, 1.166...   \n",
       "3  [0.8976665940861429, 1.3266930162536887, 1.363...   \n",
       "4  [1.0562178880200261, 1.2187872576021745, 1.227...   \n",
       "\n",
       "                        Neigbor  \n",
       "0  [72 39 59 73  4 44 45 41 22]  \n",
       "1  [ 4 44 45 41 22 61 54 31 76]  \n",
       "2  [78 35  4 33  8 68 51 66 44]  \n",
       "3  [ 1  4 44 45 41 22 61 54 31]  \n",
       "4  [ 4 44 37 22 28 54 31 33 55]  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avearge accuracy :  70.6364%\n",
      "Max Score :  90.0 in Fold :  5\n",
      "Min Score :  30.0 in Fold :  3\n"
     ]
    }
   ],
   "source": [
    "new_frame = pd.DataFrame(X_test)\n",
    "new_frame = new_frame.join(knn_pred)\n",
    "\n",
    "avg_acc = sum_accuracy/fold_n\n",
    "maxs = max(acc)\n",
    "mins = min(acc)\n",
    "res_df = pd.DataFrame({'iterasi':fol, 'Accuracy': acc, 'Precison':pr, 'Recall':rc, 'f1 score':f1})\n",
    "print(\"Avearge accuracy : \", str(\"%.4f\" % avg_acc)+'%')\n",
    "print(\"Max Score : \",str(maxs),\"in Fold : \", str(acc.index(maxs)+1))\n",
    "print(\"Min Score : \",str(mins), \"in Fold : \", str(acc.index(mins)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterasi</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precison</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>72.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iterasi   Accuracy  Precison  Recall  f1 score\n",
       "0        1  63.636364       0.0     0.0       0.0\n",
       "1        2  72.727273       0.0     0.0       0.0\n",
       "2        3  30.000000       0.0     0.0       0.0\n",
       "3        4  70.000000       0.0     0.0       0.0\n",
       "4        5  90.000000       0.0     0.0       0.0\n",
       "5        6  60.000000       0.0     0.0       0.0\n",
       "6        7  90.000000       0.0     0.0       0.0\n",
       "7        8  80.000000       0.0     0.0       0.0\n",
       "8        9  60.000000       0.0     0.0       0.0\n",
       "9       10  90.000000       0.0     0.0       0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[['Accuracy', 'Precison', 'Recall', 'f1 score']].plot.line(title=\"Akurasi tiap Fold\")\n",
    "plot.show(block=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "575b1b4c9cb40b60b65888ca604b6df5d3b9d69fd4f2756bb0ac61f69d2ddd9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
