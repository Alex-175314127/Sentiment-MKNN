{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models.MKNN import ModifiedKNN\n",
    "import neattext.functions as nfx\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from heapq import nsmallest as nMin\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Twitter_fresh/twitter_crawling.csv',encoding='latin1', usecols=['date','text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casefolding(Text):\n",
    "    Text = Text.lower()\n",
    "    return Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(casefolding)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_clean(Text):\n",
    "        Text = nfx.remove_urls(Text)\n",
    "        Text = nfx.remove_punctuations(Text)\n",
    "        Text = nfx.remove_emojis(Text)\n",
    "        Text = nfx.remove_special_characters(Text)\n",
    "        Text = nfx.remove_numbers(Text)\n",
    "        return Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(punc_clean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize_wrapper(Text):\n",
    "        return word_tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(word_tokenize_wrapper)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_norm(tweets):\n",
    "    word_dict = pd.read_csv('data/indonesia_slangWords.csv')\n",
    "    norm_word_dict = {}\n",
    "    for index, row in word_dict.iterrows():\n",
    "        if row[0] not in norm_word_dict:\n",
    "            norm_word_dict[row[0]] = row[1]\n",
    "    return [norm_word_dict[term] if term in norm_word_dict else term for term in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(word_norm)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(Text):\n",
    "    stopW = stopwords.words('indonesian', 'english')\n",
    "    sw = pd.read_csv('data/stopwordbahasa.csv')\n",
    "    stopW.extend(sw)\n",
    "    remove_sw = ' '.join(Text)\n",
    "    clean_sw = [word for word in remove_sw.split() if word.lower() not in stopW]\n",
    "    return clean_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(remove_stopword)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indo_stem(Text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    result = []\n",
    "    for w in Text:\n",
    "        result.append(stemmer.stem(w))\n",
    "        result.append(\" \")\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(indo_stem)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sentiment_analysis():\n",
    "    df = pd.read_csv('Twitter_fresh/Clean Data/Twitter_clean_2000.csv', encoding='utf-8')\n",
    "\n",
    "    # LexiconVader dengan custom Lexicon(bahasa indonesia)\n",
    "    sia1A, sia1B = SentimentIntensityAnalyzer(), SentimentIntensityAnalyzer()\n",
    "    senti = SentimentIntensityAnalyzer()\n",
    "    # Hapus Default lexicon VADER\n",
    "    sia1A.lexicon.clear()\n",
    "    sia1B.lexicon.clear()\n",
    "    senti.lexicon.clear()\n",
    "\n",
    "    # Read custom Lexicon Bahasa Indonesia\n",
    "    data1A = open('data/lexicon_sentimen_negatif.txt', 'r').read()\n",
    "    data1B = open('data/lexicon_sentimen_positif.txt', 'r').read()\n",
    "    data_senti = open('data/sentiwords_id.txt', 'r').read()\n",
    "    \n",
    "    # convert lexicon to dictonary\n",
    "    insetNeg = json.loads(data1A)\n",
    "    insetPos = json.loads(data1B)\n",
    "    sa = json.loads(data_senti)\n",
    "\n",
    "    # update lexicon vader with custom lexicon (b.indo)\n",
    "    sia1A.lexicon.update(insetNeg)\n",
    "    sia1B.lexicon.update(insetPos)\n",
    "    senti.lexicon.update(sa)\n",
    "\n",
    "    # method untuk cek apa sentimen pos,neg,neu\n",
    "    def is_positive_inset(Text: str) -> bool:\n",
    "        return sia1A.polarity_scores(Text)[\"compound\"] + sia1B.polarity_scores(Text)[\"compound\"] >= 0.05\n",
    "    \n",
    "    tweets = df['text'].to_list()\n",
    "\n",
    "    with open('output/Sentiment-result.txt', 'w+') as f:\n",
    "        for tweet in tweets:\n",
    "            label = \"Positive\" if is_positive_inset(tweet) else \"Negative\"\n",
    "            f.write(str(label + \"\\n\"))\n",
    "\n",
    "    sen = pd.read_csv('output/Sentiment-result.txt', names=['Sentiment'])\n",
    "    df = df.join(sen)\n",
    "\n",
    "    ## Save clean Dataset\n",
    "    #df.to_csv('CleanText_Sentiment.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_result = Sentiment_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_result['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_result.to_csv('output/Sentiment_result.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('output/sentiment_result_2000.csv', encoding='utf-8')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfeauter = data['text']\n",
    "ylabel = data['Sentiment']\n",
    "print(Xfeauter.shape, ylabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(decode_error='replace')\n",
    "text_tf = tf.fit_transform(Xfeauter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(ylabel.value_counts(),labels=['Label 0 (Negative Tweets)', 'Label 1 (Positive Tweets)'],autopct='%0.1f%%')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "label_cv = lbl.fit_transform(ylabel)\n",
    "print(label_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE()\n",
    "text_smote, label_smote = smt.fit_resample(text_tf, label_cv)\n",
    "print(text_smote.shape, label_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(pd.value_counts(label_smote),labels=['Label 0 (Negative Tweets)', 'Label 1 (Positive Tweets)'],autopct='%0.1f%%')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models Algorithm M KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF_word_weight(vect, word_weight):\n",
    "    feature_name = np.array(vect.get_feature_names_out())\n",
    "    data = word_weight.data\n",
    "    indptr = word_weight.indptr\n",
    "    indices = word_weight.indices\n",
    "    n_docs = word_weight.shape[0]\n",
    "\n",
    "    word_weght_list = []\n",
    "    for i in range(n_docs):\n",
    "        doc = slice(indptr[i], indptr[i + 1])\n",
    "        count, idx = data[doc], indices[doc]\n",
    "        feature = feature_name[idx]\n",
    "        word_weght_dict = dict(dict(zip(feature, count)))\n",
    "        word_weght_list.append(word_weght_dict)\n",
    "    word_weght_df = pd.DataFrame(word_weght_list)\n",
    "    word_weght_df = word_weght_df.fillna(0)\n",
    "    return word_weght_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(decode_error=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfeatures = sentiment_result.text\n",
    "Xfeatures = tf.fit_transform(Xfeatures)\n",
    "df_tfidf = TFIDF_word_weight(tf, Xfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf.to_csv('output/tfidf_res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jarak_euc(x,y):\n",
    "    return euclidean_distances(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = jarak_euc(Xfeatures,Xfeatures)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(X_train)\n",
    "dd.to_csv('euc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_result = pd.read_csv('Twitter_fresh/Balance/1000_DATA.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative    560\n",
      "Positive    440\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_result['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_value = 9 # 1 - 25\n",
    "X = sentiment_result['text'].values\n",
    "y = sentiment_result['Sentiment'].values\n",
    "fold_i = 1\n",
    "fold_n = 10 # 3 5 7 10\n",
    "sum_accuracy = 0\n",
    "kfold = KFold(fold_n, shuffle=True, random_state=42)\n",
    "enc = LabelEncoder()\n",
    "fol = []\n",
    "acc, rc, pr, f1 = [], [], [], []\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    fol.append(fold_i)\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    svf = open('output/ResultX.txt', 'w')\n",
    "    sv_text = '\\n'.join(str(item) for item in X_test).replace(\"   \",\" \")\n",
    "    svf.write(sv_text)\n",
    "    svY = open ('output/y_train.txt', 'w')\n",
    "    svY.write('\\n'.join(str(item) for item in y_train))\n",
    "\n",
    "    #TFIDF\n",
    "    tf = TfidfVectorizer(decode_error=\"replace\")\n",
    "    X_train = tf.fit_transform(X_train)\n",
    "    X_test = tf.transform(X_test)\n",
    "    \n",
    "    # label Encode\n",
    "    y_train = enc.fit_transform(y_train)\n",
    "    y_test = enc.transform(y_test)\n",
    "    \n",
    "    # Balance\n",
    "    # smt = SMOTE()\n",
    "    # X_train, y_train = smt.fit_resample(X_train, y_train)\n",
    "    # print(y_train.shape)\n",
    "\n",
    "    # Algorithm\n",
    "    clf = ModifiedKNN(k_value)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred, jarak = clf.predict(X_test)\n",
    "    neigbor_index = clf.get_neigbors(X_test)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    #tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    accuracy = accuracy_score(y_test, pred)*100\n",
    "    precision = precision_score(y_test, pred)*100\n",
    "    recall = recall_score(y_test, pred)*100\n",
    "    f1_scores = f1_score(y_test, pred)*100\n",
    "    #accuracy = (tp + tn) / (tp + fp + tn + fn)*100\n",
    "    #precision = (tp) / (tp + fp)*100\n",
    "    #recall = (tp) / (tp + fn)*100\n",
    "    #f1_scores = (2 * precision * recall) / (precision + recall)\n",
    "    #plot_conf_metrics(y_test, pred)\n",
    "\n",
    "    sum_accuracy += accuracy\n",
    "    pred = enc.inverse_transform(pred)\n",
    "\n",
    "    fold_i += 1\n",
    "    acc.append(accuracy)\n",
    "    pr.append(precision)\n",
    "    rc.append(recall)\n",
    "    f1.append(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/MKNN_prediction.txt\", \"w\") as f:\n",
    "    mknn_predited_label ='\\n'.join(str(item) for item in pred)\n",
    "    f.write(mknn_predited_label)\n",
    "with open('output/jarak_ttg.txt', 'w') as g:\n",
    "    jarak = [nMin(k_value,map(float,i)) for i in jarak]\n",
    "    mknn_distance = '\\n'.join(str(ls) for ls in jarak)\n",
    "    g.write(mknn_distance)\n",
    "with open('output/index_ttg.txt', 'w') as j:\n",
    "    j.write('\\n'.join(str(a) for a in neigbor_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = pd.read_csv('output/MKNN_prediction.txt', names=['Sentiment'])\n",
    "jarak_pred = pd.read_csv('output/jarak_ttg.txt', names=['Distance'], sep='\\t')\n",
    "text_test = pd.read_csv('output/ResultX.txt', names=['text'])\n",
    "index_pred = pd.read_csv('output/index_ttg.txt', names=['Neigbor'])\n",
    "text_test = text_test.join(knn_pred)\n",
    "text_test = text_test.join(jarak_pred)\n",
    "text_test = text_test.join(index_pred)\n",
    "#text_test['Sentiment'] = text_test['Sentiment'].apply(lambda x: 'Positive' if x == 1 else 'Negative')\n",
    "text_test = text_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Neigbor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kdma teman nonton money heist korea epsiode ga...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[1.0858738698060917, 1.128501310566763, 1.1463...</td>\n",
       "      <td>[234 858   3 883 878 853 208  84  52]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>expectation money heist original</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[1.0181489878838212, 1.0181489878838212, 1.039...</td>\n",
       "      <td>[311 310  68 309 471 418 129 331 866]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>senin baca review money heist versi korea pena...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[1.0551266547638418, 1.062683298682043, 1.1187...</td>\n",
       "      <td>[402 326 254 161 435 208 106 751 588]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weh produksi money heist money heist</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[1.008169535269051, 1.0202728522824542, 1.0534...</td>\n",
       "      <td>[239  47 222 259 272 127 328 271 409]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inget ending film money heist keruk bagiin rak...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[1.093625820481299, 1.093625820481299, 1.11537...</td>\n",
       "      <td>[666 650 174 247  98 258 205 374 677]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text Sentiment  \\\n",
       "0  kdma teman nonton money heist korea epsiode ga...  Negative   \n",
       "1                  expectation money heist original   Negative   \n",
       "2  senin baca review money heist versi korea pena...  Negative   \n",
       "3              weh produksi money heist money heist   Negative   \n",
       "4  inget ending film money heist keruk bagiin rak...  Positive   \n",
       "\n",
       "                                            Distance  \\\n",
       "0  [1.0858738698060917, 1.128501310566763, 1.1463...   \n",
       "1  [1.0181489878838212, 1.0181489878838212, 1.039...   \n",
       "2  [1.0551266547638418, 1.062683298682043, 1.1187...   \n",
       "3  [1.008169535269051, 1.0202728522824542, 1.0534...   \n",
       "4  [1.093625820481299, 1.093625820481299, 1.11537...   \n",
       "\n",
       "                                 Neigbor  \n",
       "0  [234 858   3 883 878 853 208  84  52]  \n",
       "1  [311 310  68 309 471 418 129 331 866]  \n",
       "2  [402 326 254 161 435 208 106 751 588]  \n",
       "3  [239  47 222 259 272 127 328 271 409]  \n",
       "4  [666 650 174 247  98 258 205 374 677]  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avearge accuracy :  58.6000%\n",
      "Max Score :  72.0 in Fold :  5\n",
      "Min Score :  53.0 in Fold :  3\n"
     ]
    }
   ],
   "source": [
    "new_frame = pd.DataFrame(X_test)\n",
    "new_frame = new_frame.join(knn_pred)\n",
    "\n",
    "avg_acc = sum_accuracy/fold_n\n",
    "maxs = max(acc)\n",
    "mins = min(acc)\n",
    "res_df = pd.DataFrame({'iterasi':fol, 'Accuracy': acc, 'Precison':pr, 'Recall':rc, 'f1 score':f1})\n",
    "print(\"Avearge accuracy : \", str(\"%.4f\" % avg_acc)+'%')\n",
    "print(\"Max Score : \",str(maxs),\"in Fold : \", str(acc.index(maxs)+1))\n",
    "print(\"Min Score : \",str(mins), \"in Fold : \", str(acc.index(mins)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterasi</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precison</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>59.0</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>31.914894</td>\n",
       "      <td>42.253521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>62.068966</td>\n",
       "      <td>41.860465</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.396226</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>49.462366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>53.0</td>\n",
       "      <td>38.235294</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>35.616438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>72.0</td>\n",
       "      <td>69.767442</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>68.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>56.0</td>\n",
       "      <td>44.117647</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>40.540541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>54.0</td>\n",
       "      <td>44.186047</td>\n",
       "      <td>46.341463</td>\n",
       "      <td>45.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>55.0</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>21.818182</td>\n",
       "      <td>34.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>59.0</td>\n",
       "      <td>90.909091</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>32.786885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>61.0</td>\n",
       "      <td>50.980392</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iterasi  Accuracy   Precison     Recall   f1 score\n",
       "0        1      59.0  62.500000  31.914894  42.253521\n",
       "1        2      64.0  62.068966  41.860465  50.000000\n",
       "2        3      53.0  43.396226  57.500000  49.462366\n",
       "3        4      53.0  38.235294  33.333333  35.616438\n",
       "4        5      72.0  69.767442  66.666667  68.181818\n",
       "5        6      56.0  44.117647  37.500000  40.540541\n",
       "6        7      54.0  44.186047  46.341463  45.238095\n",
       "7        8      55.0  85.714286  21.818182  34.782609\n",
       "8        9      59.0  90.909091  20.000000  32.786885\n",
       "9       10      61.0  50.980392  65.000000  57.142857"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.Accuracy.to_csv('output/res_df.txt', index=False)\n",
    "res_df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df[['Accuracy', 'Precison', 'Recall', 'f1 score']].plot.line(title=\"Akurasi tiap Fold\")\n",
    "plot.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Algorithm K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_value = 9 # 1 - 25\n",
    "X = sentiment_result['text'].values\n",
    "y = sentiment_result['Sentiment'].values\n",
    "fold_i = 1\n",
    "fold_n = 10 # 3 5 7 10\n",
    "sum_accuracy = 0\n",
    "kfold = KFold(fold_n, shuffle=True, random_state=42)\n",
    "enc = LabelEncoder()\n",
    "fol = []\n",
    "acc, rc, pr, f1 = [], [], [], []\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    fol.append(fold_i)\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    svf = open('output/ResultX.txt', 'w')\n",
    "    sv_text = '\\n'.join(str(item) for item in X_test).replace(\"   \",\" \")\n",
    "    svf.write(sv_text)\n",
    "    svY = open ('output/y_train.txt', 'w')\n",
    "    svY.write('\\n'.join(str(item) for item in y_train))\n",
    "\n",
    "    #TFIDF\n",
    "    tf = TfidfVectorizer(decode_error=\"replace\")\n",
    "    X_train = tf.fit_transform(X_train)\n",
    "    X_test = tf.transform(X_test)\n",
    "    \n",
    "    y_train = enc.fit_transform(y_train)\n",
    "    y_test = enc.transform(y_test)\n",
    "\n",
    "    # Algorithm\n",
    "    clf = KNeighborsClassifier(k_value)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    #tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    accuracy = accuracy_score(y_test, pred)*100\n",
    "    precision = precision_score(y_test, pred)*100\n",
    "    recall = recall_score(y_test, pred)*100\n",
    "    f1_scores = f1_score(y_test, pred)*100\n",
    "    #accuracy = (tp + tn) / (tp + fp + tn + fn)*100\n",
    "    #precision = (tp) / (tp + fp)*100\n",
    "    #recall = (tp) / (tp + fn)*100\n",
    "    #f1_scores = (2 * precision * recall) / (precision + recall)\n",
    "    #plot_conf_metrics(y_test, pred)\n",
    "\n",
    "    sum_accuracy += accuracy\n",
    "    pred = enc.inverse_transform(pred)\n",
    "\n",
    "    fold_i += 1\n",
    "    acc.append(accuracy)\n",
    "    pr.append(precision)\n",
    "    rc.append(recall)\n",
    "    f1.append(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/MKNN_prediction.txt\", \"w\") as f:\n",
    "    mknn_predited_label ='\\n'.join(str(item) for item in pred)\n",
    "    f.write(mknn_predited_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = pd.read_csv('output/MKNN_prediction.txt', names=['Sentiment'])\n",
    "text_test = pd.read_csv('output/ResultX.txt', names=['text'])\n",
    "text_test = text_test.join(knn_pred)\n",
    "#text_test['Sentiment'] = text_test['Sentiment'].apply(lambda x: 'Positive' if x == 1 else 'Negative')\n",
    "text_test = text_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_frame = pd.DataFrame(X_test)\n",
    "new_frame = new_frame.join(knn_pred)\n",
    "\n",
    "avg_acc = sum_accuracy/fold_n\n",
    "maxs = max(acc)\n",
    "mins = min(acc)\n",
    "res_df = pd.DataFrame({'iterasi':fol, 'Accuracy': acc, 'Precison':pr, 'Recall':rc, 'f1 score':f1})\n",
    "print(\"Avearge accuracy : \", str(\"%.4f\" % avg_acc)+'%')\n",
    "print(\"Max Score : \",str(maxs),\"in Fold : \", str(acc.index(maxs)+1))\n",
    "print(\"Min Score : \",str(mins), \"in Fold : \", str(acc.index(mins)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv('hs.txt', index=False)\n",
    "res_df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "575b1b4c9cb40b60b65888ca604b6df5d3b9d69fd4f2756bb0ac61f69d2ddd9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
